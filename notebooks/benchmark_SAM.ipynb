{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark segment anything on validation set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment anything on brain segmentation dataset\n",
    "\n",
    "To run this notebook, following instruction at https://github.com/facebookresearch/segment-anything to install dependency and backbone model.\n",
    "\n",
    "Model weights should be placed at `./weights/sam_vit_b_01ec64.pth` for example.\n",
    "\n",
    "This notebook has following sections:\n",
    "\n",
    "1. Benchmark Segment Anything model with central prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "      \n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from dataset import data_loaders, BrainSegmentationDataset\n",
    "from utils import postprocess_per_volume, dsc_distribution, plot_dsc, gray2rgb, outline\n",
    "from skimage.io import imsave, imshow\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader for validation set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading validation images...\n",
      "preprocessing validation volumes...\n",
      "cropping validation volumes...\n",
      "padding validation volumes...\n",
      "resizing validation volumes...\n",
      "normalizing validation volumes...\n",
      "done creating validation dataset\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "workers = 2\n",
    "weights = \"./\"\n",
    "image_size = 224\n",
    "aug_scale = 0.05\n",
    "aug_angle = 15\n",
    "\n",
    "_, loader_valid = data_loaders(batch_size, workers, image_size, aug_scale, aug_angle, path=\"../kaggle_3m\", valid_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Segment Anything Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"../weights/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use central point as input\n",
    "\n",
    "The original size is (224, 224), so central point is (112, 112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = np.array([[112, 112]])\n",
    "input_label = np.array([1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Segment Anything Model on `loader_vaild`\n",
    "\n",
    "Important shapes:\n",
    "\n",
    "x: (16, 3, 224, 224), where 3 is channel number\n",
    "\n",
    "y_true: (16, 1, 224, 224)\n",
    "\n",
    "16 is default batch size, which can be manipulated by `batch_size` argument in `data_loaders` function.\n",
    "\n",
    "\n",
    "**Notice that the range of y_true entry is [0, 255]**, where 0 is background (black), 255 is mask (white, groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = []\n",
    "pred_list = []\n",
    "true_list = []\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[j].shape = torch.Size([3, 224, 224])\n",
      "im_now.shape = (224, 224, 3)\n",
      "im_now = array([[[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]])\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::(anonymous namespace)::CvtHelper<cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<0, 2, 5>, cv::impl::(anonymous namespace)::NONE>::CvtHelper(cv::InputArray, cv::OutputArray, int) [VScn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDcn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDepth = cv::impl::(anonymous namespace)::Set<0, 2, 5>, sizePolicy = cv::impl::(anonymous namespace)::NONE]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 4 (CV_32S)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mim_now\u001b[39m.\u001b[39mshape\u001b[39m \u001b[39m\u001b[39m= }\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mim_now\u001b[39m \u001b[39m\u001b[39m= }\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m im_now \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(im_now, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB)\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mim_now\u001b[39m.\u001b[39mshape\u001b[39m \u001b[39m\u001b[39m= }\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m predictor\u001b[39m.\u001b[39mset_image(im_now)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::(anonymous namespace)::CvtHelper<cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<3, 4, -1>, cv::impl::(anonymous namespace)::Set<0, 2, 5>, cv::impl::(anonymous namespace)::NONE>::CvtHelper(cv::InputArray, cv::OutputArray, int) [VScn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDcn = cv::impl::(anonymous namespace)::Set<3, 4, -1>, VDepth = cv::impl::(anonymous namespace)::Set<0, 2, 5>, sizePolicy = cv::impl::(anonymous namespace)::NONE]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 4 (CV_32S)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i, data in enumerate(loader_valid):\n",
    "    x, y_true = data\n",
    "    x, y_true = x.to(device), y_true.to(device)\n",
    "\n",
    "    for j in range(batch_size):\n",
    "    \n",
    "        with torch.set_grad_enabled(False):\n",
    "            print(f\"{x[j].shape = }\")\n",
    "            # load image into predictor\n",
    "            im_now = np.transpose(x[j], (1, 2, 0))\n",
    "            im_now = im_now.detach().cpu().numpy().astype(int)\n",
    "\n",
    "            print(f\"{im_now.shape = }\")\n",
    "\n",
    "            print(f\"{im_now = }\")\n",
    "\n",
    "            im_now = cv2.cvtColor(im_now, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            print(f\"{im_now.shape = }\")\n",
    "            predictor.set_image(im_now)\n",
    "\n",
    "            # predict masks\n",
    "            masks_central, score_centrals, logits_central = predictor.predict(\n",
    "                point_coords=input_point,\n",
    "                point_labels=input_label,\n",
    "                multimask_output=True,\n",
    "            )\n",
    "            \n",
    "            # choose the mask with largest score as prediction\n",
    "            print(f\"{masks_central.shape = }\")\n",
    "            y_pred = masks_central[0] # (3, 224, 224)\n",
    "            # y_pred = np.transpose(y_pred, (1, 2, 0)) # (3, 224, 224) -> (224, 224, 3)\n",
    "            print(f\"y_pred.shape: {y_pred.shape}\")\n",
    "\n",
    "            # y_pred_np = y_pred.detach().cpu().numpy()\n",
    "            pred_list.extend([y_pred_np[s] for s in range(y_pred_np.shape[0])])\n",
    "            y_true_np = y_true.detach().cpu().numpy()\n",
    "            true_list.extend([y_true_np[s] for s in range(y_true_np.shape[0])])\n",
    "            x_np = x.detach().cpu().numpy()\n",
    "            input_list.extend([x_np[s] for s in range(x_np.shape[0])])\n",
    "        \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "545proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
